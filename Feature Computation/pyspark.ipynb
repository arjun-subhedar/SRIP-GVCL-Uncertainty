{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7458347,"sourceType":"datasetVersion","datasetId":4341413}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_csv_point_cloud(csv_file):\n    data = pd.read_csv(csv_file, header=None)\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_file = \"/kaggle/input/vaihingen/area2_cov_multi.csv\"\npoints = read_csv_point_cloud(csv_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"points[['x', 'y', 'z', 'cl', 'cs', 'cp', 'label']] = points[0].str.split(' ', expand=True)\npoints","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"points.drop(columns=[0, 'cl', 'cs', 'cp', 'label'], inplace=True)\npoints = points.astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# points = points[0:10000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"points.to_csv('points.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, FloatType\nimport numpy as np\nfrom scipy.spatial import KDTree\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"PointCloudEigenfeatures\") \\\n    .getOrCreate()\n\n# Define the schema to ignore the first empty column\nschema = StructType([\n    StructField(\"_c0\", FloatType(), True),\n    StructField(\"x\", FloatType(), True),\n    StructField(\"y\", FloatType(), True),\n    StructField(\"z\", FloatType(), True)\n])\n\n# Load your point cloud data with the defined schema\ndf = spark.read.csv(\"/kaggle/working/points.csv\", header=True, schema=schema)\n\n# Drop the unnecessary first column\ndf = df.drop(\"_c0\")\n\n# Convert DataFrame to NumPy array\npoint_cloud = np.array(df.collect())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"broadcast_point_cloud = spark.sparkContext.broadcast(point_cloud)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.spatial import KDTree\ndef compute_eigenfeatures(point):\n    # Extract the point cloud from the broadcast variable\n    point_cloud = broadcast_point_cloud.value\n    \n    # Create a KDTree for efficient neighbor search\n    kdtree = KDTree(point_cloud)\n    \n    # Find neighbors within the radius\n    radius = 0.25\n    indices = kdtree.query_ball_point(point, radius)\n    \n    # Get the neighbors\n    neighbors = point_cloud[indices]\n    \n    # Compute eigenfeatures (eigenvalues of the covariance matrix)\n    if len(neighbors) > 1:\n        cov_matrix = np.cov(neighbors, rowvar=False)\n        eigenvalues, _ = np.linalg.eigh(cov_matrix)\n    else:\n        # Handle edge cases where neighbors are less than 2\n        eigenvalues = np.array([0, 0, 0])\n    \n    return (point.tolist(), eigenvalues.tolist())\n\n# Convert the DataFrame to an RDD for parallel processing\nrdd = df.rdd.map(lambda row: np.array([row['x'], row['y'], row['z']]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the eigenfeature computation in parallel\neigenfeatures_rdd = rdd.map(compute_eigenfeatures)\n\n# Collect the results\nresults = eigenfeatures_rdd.collect()\n\n# Convert results to a DataFrame for further processing or saving\nresult_df = spark.createDataFrame(results, schema=[\"point\", \"eigenvalues\"])\nresult_df.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df = result_df.toPandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df = np.array(results_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, FloatType\n\ndef compute_covariance_eigenvalues(neighbors):\n    if len(neighbors) < 3:\n        return [0.0, 0.0, 0.0]  # Not enough points to define a plane\n    \n    points = np.array(neighbors)\n    cov_matrix = np.cov(points.T)\n    eigenvalues = np.linalg.eigvalsh(cov_matrix)\n    \n    return sorted(eigenvalues.tolist())\n\n# Register the UDF\ncompute_eigenvalues_udf = udf(compute_covariance_eigenvalues, ArrayType(FloatType()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import col, collect_list, lit\n\n# Define the radius for neighborhood search\nradius = 0.25\n\n# Broadcast the point cloud data\npoint_cloud_broadcast = spark.sparkContext.broadcast(point_cloud_df.collect())\n\ndef get_neighbors(point, radius, all_points):\n    px, py, pz = point\n    neighbors = []\n    for row in all_points:\n        x, y, z, label = row\n        if np.sqrt((x - px) ** 2 + (y - py) ** 2 + (z - pz) ** 2) <= radius:\n            neighbors.append([x, y, z])\n    return neighbors\n\n# UDF to find neighbors\ndef get_neighbors_udf(px, py, pz):\n    all_points = point_cloud_broadcast.value\n    neighbors = get_neighbors([px, py, pz], radius, all_points)\n    return neighbors\n\n# Register the UDF\nfind_neighbors_udf = udf(get_neighbors_udf, ArrayType(ArrayType(FloatType())))\n\n# Add neighbors to the DataFrame\npoint_cloud_with_neighbors = point_cloud_df.withColumn(\"neighbors\", find_neighbors_udf(col(\"x\"), col(\"y\"), col(\"z\")))\n\n# Compute eigenvalues for each point's neighborhood\npoint_cloud_with_eigen = point_cloud_with_neighbors.withColumn(\"eigenvalues\", compute_eigenvalues_udf(col(\"neighbors\")))\n\n# Show result\npoint_cloud_with_eigen.select(\"x\", \"y\", \"z\", \"eigenvalues\").show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}