{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8948890,"sourceType":"datasetVersion","datasetId":5385235},{"sourceId":8993126,"sourceType":"datasetVersion","datasetId":5416828},{"sourceId":8993346,"sourceType":"datasetVersion","datasetId":5416980},{"sourceId":9121936,"sourceType":"datasetVersion","datasetId":5506630}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tf_keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/14features-010-jak/features_010_jakteristics_14.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop(['Unnamed: 0'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.reset_index()\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncolumns_to_scale = ['z', 'eigenvalue_sum', 'omnivariance', 'eigenentropy',\n       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz']\n\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(data[columns_to_scale])\nscaled_df = pd.DataFrame(scaled_data, columns=columns_to_scale)\ndata[columns_to_scale] = scaled_df\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x = data[['Column1','Column2','Column3','Column4','Column5','Column6','Column7','Column8']]\nX = data[['z', 'eigenvalue_sum', 'omnivariance', 'eigenentropy',\n       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz']]\n\ny = data[['label']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_probability as tfp\nimport numpy as np\nimport tf_keras\n\nnum_classes = 4\nclasses_present = [1, 2, 5, 8]\nclass_mapping = {cls: i for i, cls in enumerate(classes_present)}\ny_mapped = y_train['label'].map(class_mapping)\ny_onehot = tf.one_hot(y_mapped, depth=num_classes)\n\n\ndef create_bnn(input_shape, output_shape):\n    model = tf_keras.Sequential([\n        tfp.layers.DenseFlipout(64, activation='relu', input_shape=input_shape),\n        tfp.layers.DenseFlipout(64, activation='relu'),\n        tfp.layers.DenseFlipout(output_shape, activation='softmax')\n    ])\n    return model\n\ndef neg_log_likelihood(y_true, y_pred):\n    return -tf.reduce_mean(y_pred.log_prob(y_true))\n\nnum_features = 15\ninput_shape = (num_features,)\noutput_shape = 4\nmodel = create_bnn(input_shape, output_shape)\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_onehot, epochs=10, batch_size=32, validation_split=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for calculation of epistemic uncertainty\n# predictions = [model.predict(X_test) for _ in range(100)]\n# predictions = tf.stack(predictions)\n\n# # Calculate mean and variance of predictions\n# pred_mean = tf.reduce_mean(predictions, axis=0)\n# pred_variance = tf.math.reduce_variance(predictions, axis=0)\n\n# # Convert to numpy arrays for further use\n# pred_mean = pred_mean.numpy()\n# pred_variance = pred_variance.numpy()\n\n# # Example: Print predictions and uncertainties for the first test sample\n# print(\"Predicted class probabilities:\", pred_mean[0])\n# print(\"Prediction uncertainty (variance):\", pred_variance[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}