{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8948890,"sourceType":"datasetVersion","datasetId":5385235},{"sourceId":8993126,"sourceType":"datasetVersion","datasetId":5416828},{"sourceId":8993346,"sourceType":"datasetVersion","datasetId":5416980},{"sourceId":9086944,"sourceType":"datasetVersion","datasetId":5482927},{"sourceId":9088327,"sourceType":"datasetVersion","datasetId":5483938},{"sourceId":9089052,"sourceType":"datasetVersion","datasetId":5484469},{"sourceId":9089444,"sourceType":"datasetVersion","datasetId":5484754},{"sourceId":9121936,"sourceType":"datasetVersion","datasetId":5506630}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/14features-020-jak/features_020_jakteristics_14.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop(['Unnamed: 0'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.reset_index()\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped = data.groupby(data['label'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"averages = grouped.mean()\nvariances = grouped.var()\naverages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_covariance_matrix(data, regularization=0):\n    cov_matrix = np.cov(data, rowvar=False)\n    cov_matrix += regularization * np.eye(cov_matrix.shape[0])\n    return cov_matrix\ndef fit(x_train, y_train):\n    y_train = y_train.ravel()\n    m = y_train.shape[0] \n    x_train = x_train.reshape(m, -1)\n    input_feature = x_train.shape[1]\n    class_label = 9\n    mu = np.zeros((class_label, input_feature))\n    sigma = np.zeros((class_label, input_feature, input_feature))\n    phi = np.zeros(class_label)\n\n    for label in range(class_label):\n        indices = (y_train == label)\n        phi[label] = float(np.sum(indices)) / m\n        mu[label] = np.mean(x_train[indices, :], axis=0)\n        sigma[label] = compute_covariance_matrix(x_train[indices, :])\n    \n    return phi, mu, sigma","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"has_nan = data.isnull().values.any()\nhas_nan","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncolumns_to_scale = ['z', 'eigenvalue_sum', 'omnivariance', 'eigenentropy',\n       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz']\n\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(data[columns_to_scale])\nscaled_df = pd.DataFrame(scaled_data, columns=columns_to_scale)\ndata[columns_to_scale] = scaled_df\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x = data[['Column1','Column2','Column3','Column4','Column5','Column6','Column7','Column8']]\nX = data[['z', 'eigenvalue_sum', 'omnivariance', 'eigenentropy',\n       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz']]\n\ny = data[['label']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_values = X_train.values\ny_train_values = y_train.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phi, mu, sigma = fit(X_train_values, y_train_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for label in (1,2,5,8):\n    print(np.linalg.eigvals(sigma[label]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\ndef multivariate_gaussian_pdf(x, mean, cov):\n    d = mean.shape[0]\n    exponent = -0.5 * np.dot(np.dot((x - mean).T, np.linalg.inv(cov)), (x - mean))\n    prefactor = 1 / np.sqrt(((2 * np.pi) ** d )*(np.linalg.det(cov)))\n    return np.exp(exponent)*prefactor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_positive_semidefinite(matrix):\n    eigenvalues, _ = np.linalg.eig(matrix)\n    print(eigenvalues)\n    return np.all(eigenvalues >= 0)\n\nmatrix = sigma[1] \nprint(matrix)\npositive_semidefinite = is_positive_semidefinite(matrix)\nif positive_semidefinite:\n    print(\"The matrix is positive semidefinite.\")\nelse:\n    print(\"The matrix is not positive semidefinite.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def give_epistemic(X_test):\n    x_test = X_test.values\n    feature_densities = []\n    for i in range (x_test.shape[0]):\n        rel_probs = []\n        deno = 0\n        for label in (1,2,5,8):\n            x = multivariate_gaussian_pdf(x_test[i], mu[label], sigma[label])\n            deno += x\n            rel_probs.append(x)\n        probs = [x/deno for x in rel_probs]\n        feature_density = 0\n        labels = [1,2,5,8]\n        for j in range (len(labels)):\n            feature_density += phi[labels[j]]*probs[j]\n        feature_densities.append([x_test[i], feature_density])\n    epistemic_uncertainty = []\n    for i in feature_densities:\n        epistemic_uncertainty.append(1-i[1])\n    return epistemic_uncertainty","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_aleatoric(X_test, softmax_probs):\n    entropies = []\n    sum_probs = []\n    for i in range (len(softmax_probs)):\n#         sum_prob = 0\n        for j in softmax_probs[i]:\n            entropy = 0\n            if (j == 0):\n                continue\n            else:\n                entropy+= -j*np.log(j)\n#             sum_prob += j\n\n#         sum_probs.append(sum_prob)   \n        entropies.append(entropy)\n    return entropies","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_epistemic = give_epistemic(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X['epistemic'] = X_epistemic\ndata_new = pd.concat([X, y], axis=1)\ndata_new = data_new.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_new = data_new[['z', 'eigenvalue_sum', 'omnivariance', 'eigenentropy',\n       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz', 'epistemic']]\n\ny_new = data_new[['label']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dropout\n\n# normalized_uncertainty = (epistemic_uncertainty - epistemic_uncertainty.min()) / (epistemic_uncertainty.max() - epistemic_uncertainty.min())\n\n# weights = 1 - normalized_uncertainty\n\nnum_classes = 4\nclasses_present = [1, 2, 5, 8]\nclass_mapping = {cls: i for i, cls in enumerate(classes_present)}\ny_mapped = y_train['label'].map(class_mapping)\ny_onehot = tf.one_hot(y_mapped, depth=num_classes)\n\n\n# Define your neural network architecture\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation='relu', input_shape=(16,)),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(4, activation='softmax') \n])\n\n# Compile the model with the custom loss function\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_onehot, epochs=20, batch_size=32)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score\n\ny_pred_probs = model.predict(X_test)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\ny_test_mapped = y_test['label'].map(class_mapping)\ny_test_mapped = y_test_mapped.to_numpy()\naccuracy = accuracy_score(y_test_mapped, y_pred)\n\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epistemic_uncertainty = X_train['epistemic'].values\nX_train = X_train.drop(['epistemic'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dropout\n\nnormalized_uncertainty = (epistemic_uncertainty - epistemic_uncertainty.min()) / (epistemic_uncertainty.max() - epistemic_uncertainty.min())\n\nweights = 1 - normalized_uncertainty\n\nnum_classes = 4\nclasses_present = [1, 2, 5, 8]\nclass_mapping = {cls: i for i, cls in enumerate(classes_present)}\ny_mapped = y_train['label'].map(class_mapping)\ny_onehot = tf.one_hot(y_mapped, depth=num_classes)\n\ndef weighted_categorical_crossentropy(weights):\n    def loss(y_true, y_pred):\n        # Compute the categorical cross-entropy loss\n        cce = tf.keras.losses.CategoricalCrossentropy()\n        unweighted_loss = cce(y_true, y_pred)\n        \n        # Apply weights to the loss\n        weighted_loss = unweighted_loss * weights\n        return tf.reduce_mean(weighted_loss)\n    return loss\n\n# Define your neural network architecture\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation='relu', input_shape=(15,)),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(4, activation='softmax') \n])\n\n# Compile the model with the custom loss function\nmodel.compile(optimizer='adam', loss=weighted_categorical_crossentropy(weights),  metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_onehot, epochs=20, batch_size=32)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = X_test.drop(['epistemic'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score\n\ny_pred_probs = model.predict(X_test)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\ny_test_mapped = y_test['label'].map(class_mapping)\ny_test_mapped = y_test_mapped.to_numpy()\naccuracy = accuracy_score(y_test_mapped, y_pred)\n\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dropout\n\n# normalized_uncertainty = (epistemic_uncertainty - epistemic_uncertainty.min()) / (epistemic_uncertainty.max() - epistemic_uncertainty.min())\n\n# weights = 1 - normalized_uncertainty\n\nnum_classes = 4\nclasses_present = [1, 2, 5, 8]\nclass_mapping = {cls: i for i, cls in enumerate(classes_present)}\ny_mapped = y_train['label'].map(class_mapping)\ny_onehot = tf.one_hot(y_mapped, depth=num_classes)\n\n\n# Define your neural network architecture\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation='relu', input_shape=(15,)),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(4, activation='softmax') \n])\n\n# Compile the model with the custom loss function\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_onehot, epochs=20, batch_size=32)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score\n\ny_pred_probs = model.predict(X_test)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\ny_test_mapped = y_test['label'].map(class_mapping)\ny_test_mapped = y_test_mapped.to_numpy()\naccuracy = accuracy_score(y_test_mapped, y_pred)\n\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}